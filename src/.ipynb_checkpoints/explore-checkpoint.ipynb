{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOQUE 01\n",
    "# SETUP DE PROYECTO Y LIBRERÍAS\n",
    "# - Importa librerías clave para texto + ML\n",
    "# - Crea carpetas estándar (data/models)\n",
    "# - Fija reproducibilidad\n",
    "\n",
    "# comments: Basic setup for reproducible ML workflow\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Resultado esperado:\n",
    "# - Carpetas data/raw y models creadas\n",
    "# - Entorno listo para trabajar\n",
    "\n",
    "# Interpretación:\n",
    "# - Base limpia para un pipeline reproducible\n",
    "# - Estructura típica de proyecto ML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891, 3),\n",
       " ['package_name', 'review', 'polarity'],\n",
       "           package_name                                             review  \\\n",
       " 0  com.facebook.katana   privacy at least put some option appear offli...   \n",
       " 1  com.facebook.katana   messenger issues ever since the last update, ...   \n",
       " 2  com.facebook.katana   profile any time my wife or anybody has more ...   \n",
       " \n",
       "    polarity  \n",
       " 0         0  \n",
       " 1         0  \n",
       " 2         0  )"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLOQUE 02\n",
    "# CARGA DEL DATASET\n",
    "# - Lee el CSV desde la URL oficial\n",
    "# - Guarda copia local para trazabilidad\n",
    "# - Muestra forma y columnas\n",
    "\n",
    "# comments: Load dataset from official URL and persist locally\n",
    "url = \"https://raw.githubusercontent.com/4GeeksAcademy/naive-bayes-project-tutorial/main/playstore_reviews.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.to_csv(\"data/raw/playstore_reviews.csv\", index=False)\n",
    "\n",
    "df.shape, df.columns.tolist(), df.head(3)\n",
    "\n",
    "# Resultado esperado:\n",
    "# - Dataset cargado con 3 columnas\n",
    "# - Archivo guardado en data/raw/\n",
    "\n",
    "# Interpretación:\n",
    "# - Ya tienes la fuente de datos controlada\n",
    "# - Puedes reproducir el proyecto sin depender de la red\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(polarity\n",
       " 0    584\n",
       " 1    307\n",
       " Name: count, dtype: int64,\n",
       "                                               review  polarity\n",
       " 0  privacy at least put some option appear offlin...         0\n",
       " 1  messenger issues ever since the last update, i...         0\n",
       " 2  profile any time my wife or anybody has more t...         0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLOQUE 03\n",
    "# LIMPIEZA MÍNIMA Y SELECCIÓN DE VARIABLES\n",
    "# - Elimina package_name (no aporta al sentimiento)\n",
    "# - Limpia review (strip + lowercase)\n",
    "# - Maneja nulos de forma segura\n",
    "\n",
    "# comments: Keep only text + target and clean text column\n",
    "df = df.drop(columns=[\"package_name\"], errors=\"ignore\")\n",
    "\n",
    "df[\"review\"] = df[\"review\"].astype(str).str.strip().str.lower()\n",
    "df[\"polarity\"] = pd.to_numeric(df[\"polarity\"], errors=\"coerce\")\n",
    "\n",
    "df = df.dropna(subset=[\"review\", \"polarity\"]).copy()\n",
    "df[\"polarity\"] = df[\"polarity\"].astype(int)\n",
    "\n",
    "df[\"polarity\"].value_counts(), df.head(3)\n",
    "\n",
    "# Resultado esperado:\n",
    "# - Solo quedan review y polarity\n",
    "# - polarity en enteros (0/1)\n",
    "\n",
    "# Interpretación:\n",
    "# - Quitamos ruido del modelo\n",
    "# - Preparamos el texto para vectorización consistente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((712,),\n",
       " (179,),\n",
       " np.float64(0.3441011235955056),\n",
       " np.float64(0.3463687150837989))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLOQUE 04\n",
    "# SPLIT TRAIN/TEST\n",
    "# - Separa X/y\n",
    "# - Divide en train/test con random_state\n",
    "# - Mantiene proporción de clases (stratify)\n",
    "\n",
    "# comments: Train-test split with stratification for stable evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"review\"]\n",
    "y = df[\"polarity\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.mean(), y_test.mean()\n",
    "\n",
    "# Resultado esperado:\n",
    "# - Train y test creados\n",
    "# - Proporción de positivos similar en ambos\n",
    "\n",
    "# Interpretación:\n",
    "# - Evaluación más justa y estable\n",
    "# - Menos riesgo de sesgo por desbalance accidental\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8547486033519553,\n",
       " array([[112,   5],\n",
       "        [ 21,  41]]),\n",
       " '              precision    recall  f1-score   support\\n\\n           0     0.8421    0.9573    0.8960       117\\n           1     0.8913    0.6613    0.7593        62\\n\\n    accuracy                         0.8547       179\\n   macro avg     0.8667    0.8093    0.8276       179\\nweighted avg     0.8591    0.8547    0.8486       179\\n')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLOQUE 05\n",
    "# BASELINE: COUNT VECTORIZER + MULTINOMIAL NB\n",
    "# - Convierte texto a matriz de conteos\n",
    "# - Entrena MultinomialNB (ideal para conteos)\n",
    "# - Evalúa accuracy y métricas principales\n",
    "\n",
    "# comments: Baseline model with CountVectorizer + MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "pipe_mnb = Pipeline(steps=[\n",
    "    (\"vec\", CountVectorizer(stop_words=\"english\")),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_mnb.fit(X_train, y_train)\n",
    "y_pred_mnb = pipe_mnb.predict(X_test)\n",
    "\n",
    "acc_mnb = accuracy_score(y_test, y_pred_mnb)\n",
    "cm_mnb = confusion_matrix(y_test, y_pred_mnb)\n",
    "report_mnb = classification_report(y_test, y_pred_mnb, digits=4)\n",
    "\n",
    "acc_mnb, cm_mnb, report_mnb\n",
    "\n",
    "# Resultado esperado:\n",
    "# - accuracy calculada\n",
    "# - matriz de confusión + classification_report\n",
    "\n",
    "# Interpretación:\n",
    "# - Primer modelo “rápido y sólido” para sentimiento\n",
    "# - Sirve como baseline para comparar mejoras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7821229050279329,\n",
       " array([[113,   4],\n",
       "        [ 35,  27]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLOQUE 06\n",
    "# COMPARACIÓN: BERNOULLI NB (BINARIO)\n",
    "# - Usa los mismos conteos, pero en modo presencia/ausencia\n",
    "# - Ajusta binarize para forzar 0/1\n",
    "# - Compara con baseline\n",
    "\n",
    "# comments: Compare BernoulliNB (binary features) against baseline\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "pipe_bnb = Pipeline(steps=[\n",
    "    (\"vec\", CountVectorizer(stop_words=\"english\", binary=False)),\n",
    "    (\"clf\", BernoulliNB(binarize=0.0))\n",
    "])\n",
    "\n",
    "pipe_bnb.fit(X_train, y_train)\n",
    "y_pred_bnb = pipe_bnb.predict(X_test)\n",
    "\n",
    "acc_bnb = accuracy_score(y_test, y_pred_bnb)\n",
    "cm_bnb = confusion_matrix(y_test, y_pred_bnb)\n",
    "\n",
    "acc_bnb, cm_bnb\n",
    "\n",
    "# Resultado esperado:\n",
    "# - accuracy de BernoulliNB\n",
    "# - matriz de confusión para comparar\n",
    "\n",
    "# Interpretación:\n",
    "# - Verificas si “presencia de palabra” gana a “conteo”\n",
    "# - Confirmas la implementación más adecuada para texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8156424581005587,\n",
       " array([[104,  13],\n",
       "        [ 20,  42]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLOQUE 07\n",
    "# COMPARACIÓN: GAUSSIAN NB (NO IDEAL PARA CONTEOS)\n",
    "# - Vectoriza a conteos y pasa a denso\n",
    "# - Entrena GaussianNB para confirmar si rinde peor\n",
    "# - Lo usamos como prueba de criterio\n",
    "\n",
    "# comments: GaussianNB typically mismatches sparse count data; we test to confirm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "vec = CountVectorizer(stop_words=\"english\")\n",
    "Xtr_counts = vec.fit_transform(X_train).toarray()\n",
    "Xte_counts = vec.transform(X_test).toarray()\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(Xtr_counts, y_train)\n",
    "y_pred_gnb = gnb.predict(Xte_counts)\n",
    "\n",
    "acc_gnb = accuracy_score(y_test, y_pred_gnb)\n",
    "cm_gnb = confusion_matrix(y_test, y_pred_gnb)\n",
    "\n",
    "acc_gnb, cm_gnb\n",
    "\n",
    "# Resultado esperado:\n",
    "# - accuracy de GaussianNB (normalmente menor)\n",
    "# - matriz de confusión\n",
    "\n",
    "# Interpretación:\n",
    "# - Confirmas que GaussianNB no es la mejor elección para texto con conteos\n",
    "# - Decisión basada en evidencia, no en “fe”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'MultinomialNB': 0.8547486033519553,\n",
       "  'BernoulliNB': 0.7821229050279329,\n",
       "  'GaussianNB': 0.8156424581005587},\n",
       " 'MultinomialNB')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLOQUE 08\n",
    "# SELECCIÓN DEL MEJOR NB (AUTOMÁTICA)\n",
    "# - Compara accuracies\n",
    "# - Elige el mejor candidato para optimización\n",
    "# - Deja trazado “por qué” ganó\n",
    "\n",
    "# comments: Select the best Naive Bayes variant by test accuracy\n",
    "scores = {\n",
    "    \"MultinomialNB\": acc_mnb,\n",
    "    \"BernoulliNB\": acc_bnb,\n",
    "    \"GaussianNB\": acc_gnb\n",
    "}\n",
    "\n",
    "best_nb_name = max(scores, key=scores.get)\n",
    "scores, best_nb_name\n",
    "\n",
    "# Resultado esperado:\n",
    "# - Diccionario con accuracies\n",
    "# - Nombre del NB ganador\n",
    "\n",
    "# Interpretación:\n",
    "# - Te quedas con el NB más útil para este dataset\n",
    "# - Ahora sí tiene sentido optimizarlo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'clf__alpha': 0.5, 'vec__min_df': 2, 'vec__ngram_range': (1, 1)},\n",
       " np.float64(0.7176003173903427))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLOQUE 09\n",
    "# OPTIMIZACIÓN DEL MEJOR NB (GRIDSEARCH)\n",
    "# - Ajusta hiperparámetros del vectorizador + alpha\n",
    "# - Optimiza por F1 (balance precision/recall)\n",
    "# - Devuelve mejor configuración y score\n",
    "\n",
    "# comments: Grid search over vectorizer and NB smoothing to improve F1\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "pipe_opt = Pipeline(steps=[\n",
    "    (\"vec\", CountVectorizer(stop_words=\"english\")),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"vec__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"vec__min_df\": [1, 2, 5],\n",
    "    \"clf__alpha\": [0.1, 0.5, 1.0, 2.0]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipe_opt,\n",
    "    param_grid=param_grid,\n",
    "    scoring=make_scorer(f1_score),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "grid.best_params_, grid.best_score_\n",
    "\n",
    "# Resultado esperado:\n",
    "# - Mejores hiperparámetros\n",
    "# - Mejor F1 promedio en CV\n",
    "\n",
    "# Interpretación:\n",
    "# - Aumentas calidad del clasificador sin complicar el modelo\n",
    "# - CV reduce riesgo de “me fue bien por suerte”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8268156424581006,\n",
       " array([[101,  16],\n",
       "        [ 15,  47]]),\n",
       " '              precision    recall  f1-score   support\\n\\n           0     0.8707    0.8632    0.8670       117\\n           1     0.7460    0.7581    0.7520        62\\n\\n    accuracy                         0.8268       179\\n   macro avg     0.8084    0.8107    0.8095       179\\nweighted avg     0.8275    0.8268    0.8271       179\\n')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLOQUE 10\n",
    "# EVALUACIÓN FINAL DEL NB OPTIMIZADO\n",
    "# - Predice en test con el mejor estimador\n",
    "# - Reporta métricas y matriz de confusión\n",
    "# - Guarda métricas clave para el informe\n",
    "\n",
    "# comments: Final evaluation on the test set using best estimator from CV\n",
    "best_nb = grid.best_estimator_\n",
    "\n",
    "y_pred_best = best_nb.predict(X_test)\n",
    "\n",
    "acc_best = accuracy_score(y_test, y_pred_best)\n",
    "cm_best = confusion_matrix(y_test, y_pred_best)\n",
    "report_best = classification_report(y_test, y_pred_best, digits=4)\n",
    "\n",
    "acc_best, cm_best, report_best\n",
    "\n",
    "# Resultado esperado:\n",
    "# - accuracy final\n",
    "# - matriz de confusión + reporte final\n",
    "\n",
    "# Interpretación:\n",
    "# - Ya tienes tu NB “de producción” (dentro de lo razonable)\n",
    "# - Puedes justificar la mejora con métricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8100558659217877,\n",
       " array([[103,  14],\n",
       "        [ 20,  42]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLOQUE 11\n",
    "# “SI ES POSIBLE”: RANDOM FOREST SOBRE TF-IDF\n",
    "# - Usa TF-IDF (mejor señal que conteos crudos)\n",
    "# - Entrena RandomForest como alternativa “solicitada”\n",
    "# - Compara contra NB optimizado\n",
    "\n",
    "# comments: RandomForest baseline on TF-IDF features (often weaker than linear models for text)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe_rf = Pipeline(steps=[\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=\"english\")),\n",
    "    (\"rf\", RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced_subsample\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipe_rf.fit(X_train, y_train)\n",
    "y_pred_rf = pipe_rf.predict(X_test)\n",
    "\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "acc_rf, cm_rf\n",
    "\n",
    "# Resultado esperado:\n",
    "# - accuracy y matriz de confusión de RandomForest\n",
    "# - Comparación directa con NB\n",
    "\n",
    "# Interpretación:\n",
    "# - Cumples el requerimiento de “probar RandomForest”\n",
    "# - Validación práctica: en texto suele ganar NB o modelos lineales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7821229050279329, 0.8268156424581006)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLOQUE 12\n",
    "# ALTERNATIVAS PARA SUPERAR NAIVE BAYES (LINEALES)\n",
    "# - LogisticRegression y LinearSVC suelen ser top en texto\n",
    "# - Usa TF-IDF + modelo lineal\n",
    "# - Compara contra NB optimizado\n",
    "\n",
    "# comments: Strong text baselines: Logistic Regression and LinearSVC with TF-IDF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "pipe_lr = Pipeline(steps=[\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=\"english\")),\n",
    "    (\"lr\", LogisticRegression(max_iter=2000, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "pipe_svc = Pipeline(steps=[\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=\"english\")),\n",
    "    (\"svc\", LinearSVC(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "pipe_svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = pipe_lr.predict(X_test)\n",
    "y_pred_svc = pipe_svc.predict(X_test)\n",
    "\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "acc_svc = accuracy_score(y_test, y_pred_svc)\n",
    "\n",
    "acc_lr, acc_svc\n",
    "\n",
    "# Resultado esperado:\n",
    "# - Dos accuracies (LR y LinearSVC)\n",
    "# - Una o ambas suelen competir fuerte\n",
    "\n",
    "# Interpretación:\n",
    "# - Si el objetivo es “mejor score”, los lineales son candidatos serios\n",
    "# - Si el objetivo es “simple/rápido”, NB sigue siendo excelente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'NaiveBayes_Optimized': 0.8268156424581006,\n",
       "  'RandomForest_TFIDF': 0.8100558659217877,\n",
       "  'LogReg_TFIDF': 0.7821229050279329,\n",
       "  'LinearSVC_TFIDF': 0.8268156424581006},\n",
       " 'NaiveBayes_Optimized')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLOQUE 13\n",
    "# SELECCIÓN DEL MEJOR MODELO GLOBAL\n",
    "# - Compara NB optimizado vs RF vs LR vs SVC\n",
    "# - Escoge el mejor por accuracy (simple)\n",
    "# - Deja nombre del ganador\n",
    "\n",
    "# comments: Select best overall model by accuracy on test set\n",
    "all_scores = {\n",
    "    \"NaiveBayes_Optimized\": acc_best,\n",
    "    \"RandomForest_TFIDF\": acc_rf,\n",
    "    \"LogReg_TFIDF\": acc_lr,\n",
    "    \"LinearSVC_TFIDF\": acc_svc\n",
    "}\n",
    "\n",
    "best_model_name = max(all_scores, key=all_scores.get)\n",
    "all_scores, best_model_name\n",
    "\n",
    "# Resultado esperado:\n",
    "# - Diccionario con resultados\n",
    "# - Nombre del mejor modelo global\n",
    "\n",
    "# Interpretación:\n",
    "# - Tomas decisión “data-driven”\n",
    "# - Listo para guardar el mejor modelo para entrega\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/NaiveBayes_Optimized.pkl'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLOQUE 14\n",
    "# GUARDADO FINAL DEL MODELO (Y PIPELINE)\n",
    "# - Guarda el pipeline completo (vectorizador + modelo)\n",
    "# - Evita errores de “me faltó el vectorizer”\n",
    "# - Deja artefacto listo para inferencia\n",
    "\n",
    "# comments: Persist the full pipeline so preprocessing is included\n",
    "import pickle\n",
    "\n",
    "models_map = {\n",
    "    \"NaiveBayes_Optimized\": best_nb,\n",
    "    \"RandomForest_TFIDF\": pipe_rf,\n",
    "    \"LogReg_TFIDF\": pipe_lr,\n",
    "    \"LinearSVC_TFIDF\": pipe_svc\n",
    "}\n",
    "\n",
    "final_model = models_map[best_model_name]\n",
    "\n",
    "model_path = f\"models/{best_model_name}.pkl\"\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "model_path\n",
    "\n",
    "# Resultado esperado:\n",
    "# - Archivo .pkl creado en models/\n",
    "# - Ruta del modelo impresa\n",
    "\n",
    "# Interpretación:\n",
    "# - Modelo listo para reutilizar en producción o demo\n",
    "# - Tu repo queda “entregable” y profesional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
